{
    "emb_size": 4096,
    "feedforward_size": 11008,
    "hidden_size": 4096,
    "hidden_act": "silu",
    "heads_num": 32,
    "layers_num": 32,
    "dropout": 0.0,
    "data_processor": "lm",
    "max_seq_length": 8192,
    "embedding": ["word"],
    "remove_transformer_bias": true,
    "remove_attention_bias": false,
    "remove_embedding_layernorm": true,
    "rotary_position_embedding": true,
    "encoder": "transformer",
    "feed_forward": "gated",
    "mask": "causal",
    "layernorm_positioning": "pre",
    "layernorm": "rms",
    "target": ["lm"],
    "use_logn_attn": true,
    "use_dynamic_ntk": true
  }